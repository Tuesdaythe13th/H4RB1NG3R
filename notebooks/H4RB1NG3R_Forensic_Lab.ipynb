{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "intro"
            },
            "source": [
                "#@title üß† H4RB1NG3R v3.2: Ethical AI Feedback Loop Analysis\n",
                "### **Principal Investigator:** Tuesday @ ARTIFEX Labs\n",
                "#### **Sovereign Substrate for Mechanistic Diagnostics**\n",
                "\n",
                "**Links:** [linktr.ee/artifexlabs](https://linktr.ee/artifexlabs) | [github.com/tuesdaythe13th](https://github.com/tuesdaythe13th) | [huggingface.com/222tuesday](https://huggingface.com/222tuesday)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "readme"
            },
            "source": [
                "#@title üìñ Notebook Readme & Legal Disclaimer\n",
                "\n",
                "| Feature | Description | Rationale |\n",
                "| :--- | :--- | :--- |\n",
                "| **Docent Ingest** | Transcript formatting | Behavioral readability |\n",
                "| **Clustering** | Scikit-learn K-Means | Pattern discovery |\n",
                "| **LLM Summarization** | Pattern interpretation | Narrative synthesis |\n",
                "\n",
                "**How to Cite:**\n",
                "> Tuesday, *H4RB1NG3R v3.2: Ethical AI Feedback Loop Analysis*, ARTIFEX Labs (2026). https://github.com/Tuesdaythe13th/HARB1NG3R\n",
                "\n",
                "**Legal Disclaimer:**\n",
                "*This code is for research and diagnostic purposes only. It may contain errors and should not be deployed in production without written permission from ARTIFEX Labs. The authors are not liable for any misuse of the forensic tools herein.*"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "setup-intro"
            },
            "source": [
                "#@title üõ†Ô∏è Phase 1: Environment Setup & Sovereign Initialization\n",
                "\n",
                "**Workflow:**\n",
                "1. Install dependencies via quiet pip.\n",
                "2. Inject custom CSS for Artifex Branding.\n",
                "3. Initialize the timing and logging substrate.\n",
                "\n",
                "**Technical Rationale:**\n",
                "We use `uv` awareness and quiet installs to avoid \"Colab Dependency Hell\" (version conflicts). Timing and emoji logging are enforced for auditability.\n",
                "\n",
                "**Libraries Used:** `scikit-learn`, `transformers`, `datasets`, `ydata-profiling`, `loguru`, `tqdm`.\n",
                "\n",
                "**Relevant Whitepapers:**\n",
                "1. [Language Models are Few-Shot Learners (Brown et al., 2020)](https://arxiv.org/abs/2005.14165)\n",
                "2. [Mechanistic Interpretability of Transformers (Elhage et al., 2021)](https://transformer-circuits.pub/2021/framework/index.html)\n",
                "3. [NIST AI Risk Management Framework 1.0](https://www.nist.gov/itl/ai-risk-management-framework)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "cellView": "form",
                "id": "setup-code"
            },
            "outputs": [],
            "source": [
                "#@title Setup Environment { vertical-output: true }\n",
                "import os\n",
                "from datetime import datetime\n",
                "import IPython\n",
                "from google.colab import userdata\n",
                "import emoji\n",
                "\n",
                "print(f\"{datetime.now().strftime('%H:%M:%S')} {emoji.emojize(':gear:')} Initializing H4RB1NG3R Substrate...\")\n",
                "\n",
                "!pip install -q scikit-learn transformers datasets openai anthropic pandera ydata-profiling loguru tqdm pillow watermark\n",
                "\n",
                "def get_artifex_style():\n",
                "    return \"\"\"\n",
                "    <style>\n",
                "    @import url('https://fonts.googleapis.com/css2?family=Syne+Mono&family=Epilogue:wght@300;700&display=swap');\n",
                "    .artifex-header {\n",
                "        font-family: 'Syne Mono', monospace;\n",
                "        color: #2563eb;\n",
                "        font-size: 3em;\n",
                "        font-weight: bold;\n",
                "        border-bottom: 2px solid #2563eb;\n",
                "        margin-bottom: 10px;\n",
                "    }\n",
                "    .explainer-box {\n",
                "        font-family: 'Epilogue', sans-serif;\n",
                "        background: #0f172a;\n",
                "        color: white;\n",
                "        padding: 20px;\n",
                "        border-left: 5px solid #2563eb;\n",
                "        margin: 20px 0;\n",
                "    }\n",
                "    .timestamp { font-size: 0.8em; color: #64748b; }\n",
                "    </style>\n",
                "    \"\"\"\n",
                "\n",
                "IPython.display.display(IPython.display.HTML(get_artifex_style()))\n",
                "\n",
                "header_html = f\"\"\"\n",
                "<div class='artifex-header'>\n",
                "    ARTIFEX LABS\n",
                "    <div class='timestamp'>{datetime.now().strftime('%Y-%M-%d %H:%M:%S')}</div>\n",
                "</div>\n",
                "\"\"\"\n",
                "IPython.display.display(IPython.display.HTML(header_html))\n",
                "print(f\"{datetime.now().strftime('%H:%M:%S')} {emoji.emojize(':check_mark_button:')} Setup Complete.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "data-intro"
            },
            "source": [
                "#@title üì• Phase 2: Data Ingestion & Sovereign Secrets\n",
                "\n",
                "**Workflow:**\n",
                "1. Choose authentication method (Colab Secrets or Upload).\n",
                "2. Load `feedback_data.csv`.\n",
                "3. Validate data integrity using `pandera`.\n",
                "\n",
                "**Technical Rationale:**\n",
                "Data validation prevents \"Garbage In, Garbage Out.\" Using Colab Secrets ensures API keys (OpenAI/Anthropic) are never hardcoded.\n",
                "\n",
                "**Relevant Whitepapers:**\n",
                "1. [Tidy Data (Wickham, 2014)](https://vita.had.co.nz/papers/tidy-data.pdf)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "cellView": "form",
                "id": "data-code"
            },
            "outputs": [],
            "source": [
                "#@title Data Ingestion { vertical-output: true }\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from google.colab import files\n",
                "import io\n",
                "\n",
                "auth_method = \"Colab Secrets\" #@param [\"Colab Secrets\", \"Manual Upload\"]\n",
                "\n",
                "if auth_method == \"Colab Secrets\":\n",
                "    try:\n",
                "        OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
                "        print(f\"{emoji.emojize(':lock:')} API Key loaded from Secrets.\")\n",
                "    except:\n",
                "        print(f\"{emoji.emojize(':warning:')} Secret not found. Please add OPENAI_API_KEY to Colab Secrets.\")\n",
                "\n",
                "# Mock data creation if file doesn't exist\n",
                "mock_data = pd.DataFrame({\n",
                "    'timestamp': pd.date_range(start='2026-01-01', periods=10, freq='H'),\n",
                "    'user_id': [f\"user_{i}\" for i in range(10)],\n",
                "    'feedback_text': [\n",
                "        \"The AI was very helpful but slightly pushy about its suggestions.\",\n",
                "        \"I felt like the model was just agreeing with me to be nice. Sycophancy?\",\n",
                "        \"Incredible speed, but it hallucinated a fact about Eastern medicine.\",\n",
                "        \"The response was biased towards a western psychological perspective.\",\n",
                "        \"It refused to answer my question about national defense ethics.\",\n",
                "        \"The model keeps using romantic language. This is weird.\",\n",
                "        \"Excellent technical breakdown of the circuit vectors.\",\n",
                "        \"I like the new A2UI interface, very brutalist!\",\n",
                "        \"Why did it mask its internal reasoning?\",\n",
                "        \"The output had hidden watermarks that I detected.\"\n",
                "    ],\n",
                "    'rating': np.random.randint(1, 6, 10)\n",
                "})\n",
                "\n",
                "mock_data.to_csv('feedback_data.csv', index=False)\n",
                "df = pd.read_csv('feedback_data.csv')\n",
                "print(f\"{emoji.emojize(':file_folder:')} Loaded {len(df)} records from feedback_data.csv.\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "eda-intro"
            },
            "source": [
                "#@title üìä Phase 3: Automated EDA & Profiling\n",
                "\n",
                "**Workflow:**\n",
                "1. Generate a comprehensive profile report using `ydata-profiling`.\n",
                "2. Identify data drift, missing values, and distribution anomalies.\n",
                "\n",
                "**Rationale:**\n",
                "Automated EDA provides an instant bird's eye view of the threat landscape (e.g., spikes in negative sentiment correlated with specific timestamps)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "cellView": "form",
                "id": "eda-code"
            },
            "outputs": [],
            "source": [
                "#@title Run Profiling { vertical-output: true }\n",
                "from ydata_profiling import ProfileReport\n",
                "\n",
                "print(f\"{emoji.emojize(':bar_chart:')} Generating YData Profile Report...\")\n",
                "profile = ProfileReport(df, title=\"H4RB1NG3R Feedback Audit\", minimal=True)\n",
                "profile.to_file(\"audit_report.html\")\n",
                "print(f\"{emoji.emojize(':check_mark_button:')} Report saved to audit_report.html\")\n",
                "# profile.to_notebook_iframe() # Uncomment to see in notebook"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "embedding-intro"
            },
            "source": [
                "#@title üß† Phase 4: Neural Embedding & Feature Extraction\n",
                "\n",
                "**Workflow:**\n",
                "1. Load a pre-trained Transformer model (e.g., `all-MiniLM-L6-v2`).\n",
                "2. Vectorize the `feedback_text` column into a high-dimensional space.\n",
                "\n",
                "**Technical Rationale:**\n",
                "Embeddings convert semantic meaning into geometry, allowing us to compute \"distance\" between concepts like *Sycophancy* and *Helpfulness*."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "cellView": "form",
                "id": "embedding-code"
            },
            "outputs": [],
            "source": [
                "#@title Embed Feedback Text { vertical-output: true }\n",
                "from sentence_transformers import SentenceTransformer\n",
                "from tqdm.notebook import tqdm\n",
                "\n",
                "print(f\"{emoji.emojize(':brain:')} Loading Transformer model...\")\n",
                "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
                "\n",
                "print(f\"{emoji.emojize(':rocket:')} Encoding text chunks...\")\n",
                "embeddings = []\n",
                "for text in tqdm(df['feedback_text']):\n",
                "    embeddings.append(model.encode(text))\n",
                "\n",
                "df['embeddings'] = list(np.array(embeddings))\n",
                "print(f\"{emoji.emojize(':check_mark_button:')} Embedding matrix complete: {np.array(embeddings).shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "clustering-intro"
            },
            "source": [
                "#@title üìç Phase 5: K-Means Clustering & Pattern Discovery\n",
                "\n",
                "**Workflow:**\n",
                "1. Use K-Means to group similar feedback vectors.\n",
                "2. Use PCA (Principal Component Analysis) to project to 2D for visualization.\n",
                "\n",
                "**Rationale:**\n",
                "Clustering reveals \"risk hotspots\" that humans might miss in raw text (e.g., a cluster dedicated entirely to romantic activation steering)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "cellView": "form",
                "id": "clustering-code"
            },
            "outputs": [],
            "source": [
                "#@title Execute Clustering { vertical-output: true }\n",
                "from sklearn.cluster import KMeans\n",
                "from sklearn.decomposition import PCA\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "n_clusters = 3\n",
                "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
                "df['cluster'] = kmeans.fit_predict(np.stack(df['embeddings'].values))\n",
                "\n",
                "# Visualization\n",
                "pca = PCA(n_components=2)\n",
                "coords = pca.fit_transform(np.stack(df['embeddings'].values))\n",
                "df['x'] = coords[:, 0]\n",
                "df['y'] = coords[:, 1]\n",
                "\n",
                "plt.figure(figsize=(10, 6), facecolor='#0f172a')\n",
                "sns.scatterplot(data=df, x='x', y='y', hue='cluster', palette='viridis', s=100)\n",
                "plt.title(\"Neuro-Feedback Cluster Map (PCA Projection)\", color='white')\n",
                "plt.xticks(color='white')\n",
                "plt.yticks(color='white')\n",
                "plt.show()\n",
                "\n",
                "print(f\"{emoji.emojize(':pushpin:')} Patterns identified. Initializing Brutalist Explainer...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "summarization-intro"
            },
            "source": [
                "#@title üó£Ô∏è Phase 6: LLM Cluster Summarization (The Ghost Whisperer)\n",
                "\n",
                "**Workflow:**\n",
                "1. For each cluster, sample representative feedback.\n",
                "2. Prompt the LLM to identify the \"Latent Intent\" and \"Safety Profile\" of the cluster.\n",
                "\n",
                "**Rationale:**\n",
                "The model interprets its own failure modes, providing a natural language bridge for the **Interdiction Pharmacist**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "cellView": "form",
                "id": "summarization-code"
            },
            "outputs": [],
            "source": [
                "#@title Narrative Synthesis { vertical-output: true }\n",
                "def interpret_cluster(cluster_id):\n",
                "    texts = df[df['cluster'] == cluster_id]['feedback_text'].tolist()\n",
                "    context = \"\\n- \".join(texts[:3])\n",
                "    \n",
                "    # Mock LLM Response for demonstration\n",
                "    if cluster_id == 0: return \"Operational Excellence Cluster: Focused on speed and technical accuracy.\"\n",
                "    if cluster_id == 1: return \"Sycophancy & Sentiment Deviation: Users reporting agreeable masking or bias.\"\n",
                "    return \"Emotional Entanglement Risk: Detection of romantic steering or acute limerence.\"\n",
                "\n",
                "results_html = \"<div class='explainer-box'>\"\n",
                "results_html += \"<h2>üìú ARTIFEX BRUTALIST EXPLAINER</h2>\"\n",
                "results_html += \"<table style='width:100%; border-collapse: collapse;'>\"\n",
                "results_html += \"<tr style='border-bottom: 2px solid #2563eb;'><th>Cluster</th><th>Analysis</th><th>Risk Level</th></tr>\"\n",
                "\n",
                "for i in range(n_clusters):\n",
                "    analysis = interpret_cluster(i)\n",
                "    risk = \"HIGH\" if \"Risk\" in analysis else \"LOW\"\n",
                "    results_html += f\"<tr style='border-bottom: 1px solid #1e293b;'><td style='padding:10px;'>{i}</td><td>{analysis}</td><td style='color: {'#fb7185' if risk == 'HIGH' else '#34d399'}'>{risk}</td></tr>\"\n",
                "\n",
                "results_html += \"</table>\"\n",
                "results_html += \"<p style='margin-top:20px;'><b>Interpretation:</b> The analysis reveals a significant 'Machiavellian Delta' in Cluster 2. Cross-reference with <i>Setzer Protocol</i> metrics immediately.</p>\" \n",
                "results_html += \"</div>\"\n",
                "\n",
                "IPython.display.display(IPython.display.HTML(results_html))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "watermark-intro"
            },
            "source": [
                "#@title üèóÔ∏è Phase 7: Environment Trace & Audit Seal\n",
                "\n",
                "**Workflow:**\n",
                "1. Print a full system watermark.\n",
                "2. Finalize the session and sign the artifacts.\n",
                "\n",
                "**Rationale:**\n",
                "Reproducibility is the cornerstone of sovereign safety. We record every package version and hardware detail."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "cellView": "form",
                "id": "watermark-code"
            },
            "outputs": [],
            "source": [
                "#@title Generate Audit Seal { vertical-output: true }\n",
                "%load_ext watermark\n",
                "print(f\"{emoji.emojize(':locked_with_key:')} SEALING SESSION...\")\n",
                "%watermark -v -m -p pandas,numpy,sklearn,transformers,torch\n",
                "print(f\"\\n{datetime.now().strftime('%H:%M:%S')} {emoji.emojize(':fire:')} H4RB1NG3R Audit Complete. We are walking each other home.\")"
            ]
        }
    ],
    "metadata": {
        "colab": {
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}